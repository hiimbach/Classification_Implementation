{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bach/anaconda3/envs/learn/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.vit import  ViT\n",
    "from utils.train_loop import TrainingLoop\n",
    "from utils.data_loader import CustomDataset, data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, num_classes = data_split('data/mushrooms/', 0.8)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_data = CustomDataset(train_data, transform=train_transform)\n",
    "test_data = CustomDataset(val_data, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366 1347\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
      "torch.Size([54, 3, 224, 224]) torch.Size([54])\n"
     ]
    }
   ],
   "source": [
    "for img, labels in train_loader:\n",
    "    print(img.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bach/anaconda3/envs/learn/lib/python3.7/site-packages/torchvision/models/_utils.py:136: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  f\"Using {sequence_to_str(tuple(keyword_only_kwargs.keys()), separate_last='and ')} as positional \"\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "fc = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=9, bias=True)\n",
    "model.fc = fc\n",
    "\n",
    "data_path = \"data/mushrooms\"\n",
    "batch_size = 4\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optim_fn = optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = TrainingLoop(model, data_path, batch_size, loss_fn, optim_fn, 0.001, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:30:21.207820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 23:30:21.645979: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-31 23:30:21.695441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-31 23:30:21.695473: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-31 23:30:22.684774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-31 23:30:22.684848: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-31 23:30:22.684854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:30:23.758911 Start train on device cpu\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 83/1342 [20:52<5:16:43, 15.09s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 1 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32925/2715248623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Bach/Coding/Python/Pytorch/implementation/vision_transformer/utils/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, save_name, eval_interval)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Training step get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bach/Coding/Python/Pytorch/implementation/vision_transformer/utils/train_loop.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, img_path_batch, labels)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Create tensor batches from img paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mimg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bach/Coding/Python/Pytorch/implementation/vision_transformer/utils/data_loader.py\u001b[0m in \u001b[0;36mfilename_to_tensor\u001b[0;34m(files, transform)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mtensor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# tf = transforms.Compose([\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 1 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "train_task.train(10, \"test\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
