{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = 640,\n",
    "    patch_size = 32,\n",
    "    num_classes = 4,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "427\n",
      "167\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from utils.dataloader import custom_dataloader\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize([640,640]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_loader, val_loader = custom_dataloader(\"vit_ds_class\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 640, 640])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataloader import filename_to_tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize([640,640]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "sample = train_loader['img_path'][0]\n",
    "img_batch = filename_to_tensor(sample, tf)\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bach/Bach/Coding/Python/Pytorch/implementation/vision_transformer/model/vit.py:133: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "preds = model(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4692, 0.0475, 0.2349, 0.2484],\n",
       "        [0.3693, 0.0549, 0.3367, 0.2391],\n",
       "        [0.3882, 0.0438, 0.3356, 0.2324],\n",
       "        [0.2640, 0.0668, 0.3745, 0.2948],\n",
       "        [0.4960, 0.0697, 0.2708, 0.1634],\n",
       "        [0.4849, 0.0408, 0.2808, 0.1935],\n",
       "        [0.3633, 0.0473, 0.3131, 0.2762],\n",
       "        [0.4098, 0.0619, 0.2639, 0.2644],\n",
       "        [0.4545, 0.0589, 0.2846, 0.2020],\n",
       "        [0.4608, 0.0545, 0.2260, 0.2587],\n",
       "        [0.3104, 0.0512, 0.3301, 0.3083],\n",
       "        [0.3465, 0.0489, 0.3103, 0.2943],\n",
       "        [0.3844, 0.0645, 0.3144, 0.2367],\n",
       "        [0.3687, 0.0445, 0.3315, 0.2553],\n",
       "        [0.5365, 0.0457, 0.2137, 0.2041],\n",
       "        [0.4042, 0.0520, 0.3094, 0.2343]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = F.one_hot(train_loader['label'][0], 4).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(preds, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3699, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
